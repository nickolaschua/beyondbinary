# Task List — Stream 1: ws_server.py

<!-- Ralph picks the first unchecked task each iteration. TDD: write failing test, then implement. -->
<!-- SCOPE: Only modify ws_server.py and ml/tests/test_ws_*.py or ml/tests/test_*_ws.py -->

- [x] Harden decode_frame: replace bare `except Exception:` on base64.b64decode with `except (binascii.Error, ValueError):`. Add data URL comma validation — if input starts with "data:" but contains no comma, return None. Add payload size check — if len(data) > 5_000_000 (5MB), return None and log warning. In the WebSocket handler, change the outer `except Exception as e:` to log full traceback with `logger.exception()`. Write ml/tests/test_decode_frame_hardened.py testing (a) invalid base64 → None, (b) valid base64 non-image → None, (c) "data:image/jpeg;base64" with no comma → None, (d) string >5MB → None, (e) normal input still works.
- [x] Add per-client frame rate limiting: in ws_server.py sign_detection handler, add a `frame_times = deque(maxlen=60)` per connection. Before processing each frame, append `time.time()`. If `len(frame_times) == 60` and `frame_times[-1] - frame_times[0] < 10.0`, send `{"type": "error", "message": "Rate limit exceeded: max 60 frames per 10 seconds"}` and skip processing. Write ml/tests/test_ws_rate_limit.py testing (a) normal rate → all accepted, (b) burst over limit → rate limited error returned. Mock time.time for deterministic tests.
- [x] Add inference timing: in ws_server.py, wrap MediaPipe detection and model.predict() calls with `time.perf_counter()`. Add `total_inference_ms` field (rounded to 1 decimal) to the sign_prediction response JSON. Log a warning if total exceeds 200ms. Track running averages in a module-level `deque(maxlen=100)` and add `avg_inference_ms` to /health response. Write ml/tests/test_ws_timing.py verifying (a) sign_prediction response includes total_inference_ms field, (b) /health response includes avg_inference_ms field.
- [x] Migrate to lifespan: replace deprecated `@app.on_event("startup")` with an async context manager using `contextlib.asynccontextmanager` as the `lifespan` parameter to `FastAPI()`. Move model loading into the lifespan function's startup phase. Write ml/tests/test_ws_lifespan.py verifying (a) /health returns model_loaded status correctly after lifespan startup, (b) server starts without model file and /health returns model_loaded=False. Verify all existing tests still pass.
- [ ] ALL_TASKS_COMPLETE
