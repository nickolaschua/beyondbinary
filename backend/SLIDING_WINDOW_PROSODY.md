# Sliding Window Prosody Architecture

## Design Philosophy

**Speech text is fast and incremental.**  
**Emotion is slow and stateful.**  
**Optimize for perceived immediacy, not theoretical simultaneity.**

---

## What Changed

### Before (Per-Chunk Tone Analysis)
```
Audio chunk arrives (3s)
    â†“
[STT + Tone run in parallel]  â† Both must complete
    â†“ 1.5s wait â†“
Send subtitle + tone together
```

**Problem:**
- Subtitles delayed by tone analysis (1.5s total)
- Tone jitters on every chunk (unstable)
- Wasted CPU analyzing overlapping audio

### After (Sliding Window Prosody)
```
Audio chunk arrives (3s)
    â†“
Add to 2s rolling buffer  â† Instant
    â†“
Transcribe audio (STT)  â† 500ms
    â†“
Send subtitle IMMEDIATELY  â† User sees text fast!
    â†“
[Background: Analyze 2s window every 0.8s]  â† Runs in parallel
    â†“
Emit tone_update when ready  â† Separate message
```

**Benefits:**
- âœ… Subtitles appear in ~500ms (not 1.5s)
- âœ… Tone is stable (2s context, smoothing filter)
- âœ… Efficient (analyze every 0.8s, not every chunk)
- âœ… Conversation-level emotion (not per-sentence jitter)

---

## Architecture Components

### 1. ProsodyBuffer Class

**Location:** `app/services/prosody_buffer.py`

**Responsibilities:**
- Maintain rolling 2-second audio buffer
- Trigger periodic analysis every 0.8 seconds
- Smooth tone changes (persistence filter)
- Emit tone updates via callback

**Key Methods:**
```python
await buffer.append_chunk(audio_bytes)      # Add audio (non-blocking)
await buffer.trigger_analysis_if_ready()    # Check if 0.8s elapsed
current_tone = buffer.get_current_tone()    # Get latest tone state
```

### 2. Conversation Router Integration

**Location:** `app/routers/conversation.py`

**Flow:**
```python
# Initialize per-connection buffer
prosody_buffer = ProsodyBuffer(
    window_size_seconds=2.0,
    analysis_interval_seconds=0.8,
    tone_analyzer=analyze_tone_from_audio,
    on_tone_update=emit_tone_to_frontend,
)

# On audio chunk arrival:
1. await prosody_buffer.append_chunk(audio_bytes)  # Add to buffer
2. await prosody_buffer.trigger_analysis_if_ready()  # Maybe analyze
3. transcript = await transcribe_audio(audio_bytes)  # STT (fast)
4. await send_subtitle_immediately(transcript)       # Don't wait!

# Background: Tone analysis runs every 0.8s
#   â†’ Emits tone_update when complete
```

---

## Timing Diagram

```
Time:  0s      0.5s     0.8s     1.0s     1.6s     2.0s
       â”‚       â”‚        â”‚        â”‚        â”‚        â”‚
Audio: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”€â†’ buffer  buffer  buffer  buffer  buffer
       â”‚       â”‚        â”‚        â”‚        â”‚        â”‚
STT:   â””â”€â”€â”€â”€â”€â”€â†’subtitle         subtitle         subtitle
       â”‚       â”‚        â”‚        â”‚        â”‚        â”‚
Tone:          â”‚      analyze            analyze
       â”‚       â”‚        â””â”€â”€â”€â”€â”€â†’update     â””â”€â”€â”€â”€â”€â†’update
```

**Key Insight:**  
Subtitles emit every ~0.5s (STT speed).  
Tone updates emit every ~0.8s (analysis interval).  
They are **decoupled** - subtitles never wait for tone.

---

## Tone Stability Logic

Prevents jittery tone changes with a persistence filter:

```python
def _should_emit_tone(new_tone, new_confidence):
    # Always emit first tone
    if no_previous_tone:
        return True
    
    # High confidence â†’ always emit
    if new_confidence > 0.6:
        return True
    
    # Same tone persists â†’ emit after 2 windows
    if new_tone == current_tone:
        persistence_count += 1
        return persistence_count >= 2
    
    # Different tone â†’ only if moderately confident
    return new_confidence > 0.5
```

**Result:**  
Tone changes are smooth and meaningful, not flickering.

---

## WebSocket Message Flow

### Frontend â†’ Backend
```javascript
{
  type: "audio_chunk",
  audio: "<base64>",
  format: "webm"
}
```

### Backend â†’ Frontend (Two separate messages)

**Message 1: Subtitle (immediate, ~500ms)**
```javascript
{
  type: "transcript",
  text: "Your blood sugar is high",
  tone: "with concern",  // Current conversation tone
  tone_category: "concern",
  tone_confidence: 0.75
}
```

**Message 2: Tone Update (periodic, every ~0.8s)**
```javascript
{
  type: "tone_update",
  tone: "anxiously",
  tone_category: "negative",
  tone_confidence: 0.82,
  top_emotions: [...]
}
```

**Note:** Frontend should update the tone badge when `tone_update` arrives, but display subtitles immediately when `transcript` arrives.

---

## Performance Characteristics

| Metric | Before | After | Improvement |
|--------|--------|-------|-------------|
| **Subtitle Latency** | 1.5s | 0.5s | **3x faster** |
| **Tone Stability** | Jittery | Smooth | **Filtered** |
| **CPU Efficiency** | Every chunk | Every 0.8s | **~3x less** |
| **Memory Usage** | Per-chunk | 2s buffer | **Bounded** |

---

## Failure Handling

### Tone Analysis Fails
```python
if tone_result.get("success") == False:
    # Reuse last known tone
    tone = last_tone or {"primary_tone": "neutral", "confidence": 0.0}
```

### Tone Analysis Times Out
```python
# Background task continues
# Subtitles are NEVER blocked
# Next analysis cycle will retry
```

### No Audio in Buffer
```python
if len(window_audio) < 5000:  # < 0.3s of audio
    skip_analysis()  # Not enough data yet
```

---

## Configuration Tuning

Located in `conversation.py`:

```python
prosody_buffer = ProsodyBuffer(
    window_size_seconds=2.0,        # Larger = more stable, more latency
    analysis_interval_seconds=0.8,  # Smaller = more updates, more CPU
    tone_analyzer=analyze_tone_from_audio,
    on_tone_update=tone_update_callback,
)
```

**Recommended Settings:**
- **Low latency demo:** `window_size=1.5s, interval=0.6s`
- **Stable tone (default):** `window_size=2.0s, interval=0.8s`
- **Battery saving:** `window_size=2.5s, interval=1.0s`

---

## Testing

```bash
# Start server
python -m uvicorn app.main:app --reload

# Monitor logs for:
ðŸŽ­ ProsodyBuffer started (window: 2.0s, interval: 0.8s)
ðŸŽ™ï¸  Transcribing audio...
âš¡ Subtitle emitted immediately! (tone: neutral)
ðŸŽ­ Running prosody analysis on 32000 bytes (2.0s)
âœ… Tone update emitted: with concern (0.78)
```

**Expected Behavior:**
- Subtitles appear ~every 3s (chunk rate)
- Tone updates appear ~every 0.8s (analysis rate)
- Tone changes are smooth, not flickering

---

## Migration Notes

### Removed Code
- âŒ Per-chunk `tone_task = asyncio.create_task(analyze_tone_from_audio(audio_bytes))`
- âŒ `await asyncio.gather(transcript_task, tone_task)`
- âŒ Blocking tone analysis before subtitle emission

### Added Code
- âœ… `ProsodyBuffer` class with sliding window
- âœ… Periodic background tone analysis (0.8s interval)
- âœ… Tone smoothing / persistence filter
- âœ… Decoupled `tone_update` WebSocket messages

### No Changes Required
- âœ… STT logic (transcription) unchanged
- âœ… Quick-reply generation unchanged
- âœ… Frontend audio capture unchanged

---

## Design Principles (Review)

1. **Subtitle latency is critical** â†’ Never block on tone analysis
2. **Tone is conversational state** â†’ Not per-sentence granularity
3. **Smooth > Accurate** â†’ Filter out jittery tone changes
4. **Fail gracefully** â†’ Reuse last tone if analysis fails
5. **Bounded resources** â†’ Fixed buffer size, cancel on disconnect

---

## Future Enhancements (Optional)

- **Speaker diarization:** Track tone per speaker
- **Adaptive intervals:** Slower analysis when no one is speaking
- **Tone prediction:** Use transcript to pre-filter tone analysis
- **Hardware acceleration:** Use GPU for Hume if available

---

## Summary

The sliding window approach **decouples subtitle speed from tone accuracy**, giving users the best of both worlds:

- **Fast subtitles** for immediate comprehension
- **Stable tone** for reliable emotional context
- **Efficient processing** for battery life

This is the correct architecture for real-time accessibility systems.
