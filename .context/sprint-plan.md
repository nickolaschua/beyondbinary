# SenseAI â€” Beyond Binary Hackathon Sprint Plan
## 5-Person Team | 48 Hours | Pre-Recorded Video Submission

---

## What You're Building (Final Scope)

SenseAI is an adaptive accessibility companion with **two conversation modes** and a **profile-driven channel system** that switches the entire UI based on the user's ability profile.

### Mode 1: Sign Language Video Call (Deaf â†” Hearing/Blind)
A deaf user signs ASL on a video call. The app detects signs via MediaPipe + LSTM, converts to text, and speaks aloud via TTS for the other participant. The hearing participant speaks, and their speech appears as captions with emotion/tone badges for the deaf user.

### Mode 2: In-Person Conversation Intelligence (Deaf or Blind in a face-to-face setting)
A deaf user is at a doctor's appointment. The app listens to the doctor's speech, displays live captions with tone annotations and jargon simplification, and provides quick-reply buttons. When the deaf user taps a quick-reply, the app speaks it aloud naturally. A blind user in the same scenario hears contextual audio summaries through earbuds.

### Profile System
Two profiles: **Deaf** and **Visually Impaired**. Selected during a 2-screen onboarding flow. The profile determines:
- Deaf â†’ all output is visual (captions, cards, vibration). Input is touch/sign/camera. Audio output is OFF for self, ON for speaking to others.
- Visually Impaired â†’ all output is audio (TTS through earbuds). Input is voice. Screen is minimal/off.

### Emotion Layer (runs across both modes)
- Voice emotion: Hume AI speech prosody â†’ tone badges on captions ("speaking carefully", "sounds frustrated")
- Facial emotion: Hume AI facial expression â†’ emotion indicators on video call
- Both are API calls, no custom ML needed.

---

## What's CUT (confirmed)

- âŒ GPS / Google Places API environmental awareness
- âŒ Cognitive impairment simplified cards (3rd profile)
- âŒ PostgreSQL + pgvector persistent context/memory
- âŒ Camera on-demand (reading menus, forms, signs)
- âŒ Multiple sign languages (ASL only)
- âŒ Full routing engine (hardcoded 2-profile switch instead)
- âŒ Post-conversation summary with calendar/reminder integration
- âŒ Ambient sound classification (PA announcements, alarms, etc.)

---

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        BROWSER (Next.js PWA)                     â”‚
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  ONBOARDING  â”‚    â”‚           MAIN APP                       â”‚ â”‚
â”‚  â”‚  2 screens   â”‚    â”‚                                          â”‚ â”‚
â”‚  â”‚  Sets profileâ”‚â”€â”€â”€â–¶â”‚  Profile Context (React Context)         â”‚ â”‚
â”‚  â”‚  deaf/blind  â”‚    â”‚       â”‚                                  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚       â–¼                                  â”‚ â”‚
â”‚                      â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚ â”‚
â”‚                      â”‚  â”‚ MODE 1   â”‚    â”‚   MODE 2      â”‚      â”‚ â”‚
â”‚                      â”‚  â”‚ Video    â”‚    â”‚   In-Person   â”‚      â”‚ â”‚
â”‚                      â”‚  â”‚ Call     â”‚    â”‚   Conversationâ”‚      â”‚ â”‚
â”‚                      â”‚  â”‚          â”‚    â”‚               â”‚      â”‚ â”‚
â”‚                      â”‚  â”‚ PeerJS   â”‚    â”‚  "Listen"     â”‚      â”‚ â”‚
â”‚                      â”‚  â”‚ WebRTC   â”‚    â”‚   mode        â”‚      â”‚ â”‚
â”‚                      â”‚  â”‚          â”‚    â”‚               â”‚      â”‚ â”‚
â”‚                      â”‚  â”‚ Local    â”‚    â”‚  Web Speech   â”‚      â”‚ â”‚
â”‚                      â”‚  â”‚ video â”€â”€â–¶â”‚    â”‚  API (STT) â”€â”€â–¶â”‚      â”‚ â”‚
â”‚                      â”‚  â”‚ frames   â”‚    â”‚  transcript   â”‚      â”‚ â”‚
â”‚                      â”‚  â”‚ sent via â”‚    â”‚  sent to      â”‚      â”‚ â”‚
â”‚                      â”‚  â”‚ WS to    â”‚    â”‚  backend      â”‚      â”‚ â”‚
â”‚                      â”‚  â”‚ backend  â”‚    â”‚               â”‚      â”‚ â”‚
â”‚                      â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚ â”‚
â”‚                      â”‚       â”‚                  â”‚               â”‚ â”‚
â”‚                      â”‚       â–¼                  â–¼               â”‚ â”‚
â”‚                      â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚ â”‚
â”‚                      â”‚  â”‚      OUTPUT LAYER                 â”‚   â”‚ â”‚
â”‚                      â”‚  â”‚  (profile-driven rendering)       â”‚   â”‚ â”‚
â”‚                      â”‚  â”‚                                   â”‚   â”‚ â”‚
â”‚                      â”‚  â”‚  Deaf: captions + tone badges +   â”‚   â”‚ â”‚
â”‚                      â”‚  â”‚        quick-reply buttons +      â”‚   â”‚ â”‚
â”‚                      â”‚  â”‚        vibration patterns          â”‚   â”‚ â”‚
â”‚                      â”‚  â”‚                                   â”‚   â”‚ â”‚
â”‚                      â”‚  â”‚  Blind: TTS audio summaries +     â”‚   â”‚ â”‚
â”‚                      â”‚  â”‚         haptic cues                â”‚   â”‚ â”‚
â”‚                      â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚ â”‚
â”‚                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚ WebSocket
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    BACKEND (FastAPI + Python)                     â”‚
â”‚                                                                   â”‚
â”‚  WebSocket Endpoints:                                             â”‚
â”‚                                                                   â”‚
â”‚  /ws/sign-detection                                               â”‚
â”‚     Receives video frames (base64)                                â”‚
â”‚     â†’ MediaPipe Holistic (landmark extraction)                    â”‚
â”‚     â†’ LSTM model (sign classification)                            â”‚
â”‚     â†’ Returns: { sign: "hello", confidence: 0.95 }               â”‚
â”‚                                                                   â”‚
â”‚  /ws/conversation                                                 â”‚
â”‚     Receives audio chunks                                         â”‚
â”‚     â†’ Groq Whisper API (fast STT)                                 â”‚
â”‚     â†’ Hume AI Expression Measurement (tone/prosody)               â”‚
â”‚     â†’ Claude/GPT API (jargon simplification + quick-reply gen)    â”‚
â”‚     â†’ Returns: { transcript, tone, simplified, quick_replies }    â”‚
â”‚                                                                   â”‚
â”‚  REST Endpoints:                                                  â”‚
â”‚                                                                   â”‚
â”‚  POST /tts                                                        â”‚
â”‚     Receives text (quick-reply selection or sign detection)        â”‚
â”‚     â†’ ElevenLabs API (streaming TTS)                              â”‚
â”‚     â†’ Returns: audio stream                                       â”‚
â”‚                                                                   â”‚
â”‚  POST /profile                                                    â”‚
â”‚     Receives profile config from onboarding                       â”‚
â”‚     â†’ Stores in-memory (no DB needed for demo)                    â”‚
â”‚                                                                   â”‚
â”‚  ML Pipeline (loaded at server start):                            â”‚
â”‚     MediaPipe Holistic model                                      â”‚
â”‚     LSTM sign classifier (action_model.h5)                        â”‚
â”‚     30-frame sliding window buffer per connection                 â”‚
â”‚                                                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Tech Stack (Final)

| Layer | Technology | Owner |
|-------|-----------|-------|
| Frontend framework | Next.js 14 (App Router) + Tailwind | Frontend team |
| PWA setup | next-pwa, deployed on Vercel with HTTPS | Frontend 1 |
| Video call | PeerJS 1.5.5 (free cloud signaling server) | Frontend 2 |
| State management | React Context for profile + Zustand for app state | Frontend team |
| Backend framework | FastAPI (Python, async) on Railway | Backend lead |
| WebSocket | FastAPI WebSocket endpoints | Backend lead |
| STT (speech-to-text) | Groq Whisper API (fastest option) | Backend lead |
| TTS (text-to-speech) | ElevenLabs streaming API | Backend lead |
| Tone/emotion analysis | Hume AI Expression Measurement API | Backend lead |
| Jargon simplification | Claude API (or GPT-4) | Backend lead |
| Quick-reply generation | Claude API (or GPT-4) | Backend lead |
| Landmark extraction | MediaPipe Holistic (Python, legacy API) | ML lead |
| Sign classification | Keras LSTM (3-layer, tanh) | ML lead |
| Sign data format | .npy keypoint sequences (30 frames Ã— 1662 features) | ML lead |

### API Keys Needed (get these BEFORE the hackathon)
- [ ] Hume AI API key (free tier at dev.hume.ai)
- [ ] ElevenLabs API key (free tier)
- [ ] Groq API key (free tier, very fast Whisper)
- [ ] Anthropic or OpenAI API key (for Claude/GPT jargon simplification)
- [ ] Vercel account (free, for frontend deployment)
- [ ] Railway account (free tier, for backend deployment)

---

## 48-Hour Sprint â€” Per-Person Breakdown

### HOURS 0â€“2: Everyone â€” Setup & Alignment

All 5 people in the same room/call.

- Confirm everyone has dev environment working (Node.js, Python 3.10+, pip, npm)
- Clone the shared repo, set up monorepo structure:
  ```
  senseai/
  â”œâ”€â”€ frontend/          (Next.js)
  â”œâ”€â”€ backend/           (FastAPI)
  â”œâ”€â”€ ml/                (MediaPipe + LSTM training)
  â”œâ”€â”€ models/            (trained .h5 files)
  â””â”€â”€ demo/              (video assets, scripts)
  ```
- Backend lead: verify all API keys work (Hume, ElevenLabs, Groq, Claude)
- ML lead: verify MediaPipe Holistic runs on their machine, test webcam capture
- Frontend team: scaffold Next.js app, install Tailwind, PeerJS, verify Vercel deploy
- **Everyone agrees on the WebSocket message format** (this prevents integration hell later):

```json
// Sign detection response
{ "type": "sign", "sign": "hello", "confidence": 0.95 }

// Conversation response
{
  "type": "conversation",
  "transcript": "Your blood sugar is a bit high",
  "speaker": "Doctor",
  "tone": { "careful": 0.7, "concerned": 0.6 },
  "simplified": "Your sugar levels are higher than normal",
  "quick_replies": ["How serious is it?", "What should I do?", "I understand"]
}

// TTS request
{ "type": "tts", "text": "Could you tell me how serious this is?" }
```

---

### ML LEAD â€” Full 48-Hour Track

**Goal: working sign language detection pipeline, 8-10 ASL signs, >90% accuracy**

| Hours | Task | Output |
|-------|------|--------|
| 0â€“2 | Setup with team (above) | Environment ready |
| 2â€“4 | Build data collection script. Webcam â†’ MediaPipe Holistic â†’ extract keypoints â†’ save .npy. Base this on nicknochnack/ActionDetectionforSignLanguage | `collect_data.py` working |
| 4â€“8 | Record training data for 10 signs. 30 sequences Ã— 30 frames each. Signs: Hello, Thank You, Sorry, Help, Please, Yes, No, I Love You, Goodbye, More | `ml/MP_Data/` folder populated |
| 8â€“10 | Build + train LSTM model. 3-layer LSTM, tanh activation, 200 epochs | `models/action_model.h5` saved, >90% accuracy |
| 10â€“12 | Test real-time inference standalone (webcam â†’ MediaPipe â†’ LSTM â†’ print prediction). Debug false positives, tune confidence threshold | Working standalone demo |
| 12â€“16 | Build WebSocket integration: FastAPI `/ws/sign-detection` endpoint that receives base64 frames from browser, runs MediaPipe + LSTM, returns predictions. Coordinate with backend lead on the endpoint structure | WebSocket endpoint working |
| 16â€“20 | Integration testing with frontend. Frontend sends frames, receives predictions. Debug latency, frame dropping, buffer issues | End-to-end sign detection in browser |
| 20â€“24 | Add "no sign" / idle detection (model should not hallucinate signs when hands are resting). Add confidence smoothing (require N consecutive high-confidence predictions before outputting a sign) | Robust predictions, fewer false positives |
| 24â€“30 | If time: expand vocabulary to 15 signs. If not: polish existing 10 signs, improve accuracy with more training data | Final model |
| 30â€“36 | Help frontend team with sign detection UI bugs. Help record demo video segments | Support role |
| 36â€“42 | Final model freeze. Any last accuracy improvements | `action_model_final.h5` |
| 42â€“48 | Demo recording, bug fixes, buffer | Done |

**Critical path items for ML lead:**
- Data collection MUST be done by hour 8. No data = no model.
- The WebSocket frame pipeline (hour 12-16) is the hardest integration point. ML lead and backend lead should pair on this.
- Use `tanh` activation, NOT `relu` for LSTM layers.
- The confidence threshold matters more than accuracy â€” a false positive in a demo is worse than a missed sign.

---

### BACKEND LEAD â€” Full 48-Hour Track

**Goal: FastAPI server with WebSocket endpoints for sign detection, conversation intelligence, and TTS**

| Hours | Task | Output |
|-------|------|--------|
| 0â€“2 | Setup with team (above) | Environment ready |
| 2â€“5 | FastAPI skeleton: project structure, CORS, WebSocket boilerplate, health check endpoint. Deploy to Railway early (deploy-first development) | `backend/` running on Railway |
| 5â€“8 | `/ws/conversation` endpoint: receive audio â†’ Groq Whisper STT â†’ return transcript. Test with simple audio from browser | Live transcription working |
| 8â€“11 | Add Hume AI integration: send audio to Hume Expression Measurement API â†’ receive prosody/tone scores â†’ attach to transcript response. Map Hume's 48 dimensions to simplified labels ("speaking carefully", "sounds concerned", "neutral") | Tone-annotated transcripts |
| 11â€“14 | Add Claude/GPT integration: send transcript + context â†’ receive jargon simplification + 3 quick-reply suggestions. Design the prompt carefully â€” it needs to simplify medical/legal jargon while preserving meaning | Simplified text + quick replies |
| 14â€“16 | `/api/tts` endpoint: receive text â†’ ElevenLabs streaming API â†’ return audio. Test with quick-reply text | TTS working |
| 16â€“20 | `/ws/sign-detection` endpoint: coordinate with ML lead. Receive base64 frames from browser â†’ pass to MediaPipe + LSTM pipeline â†’ return predictions. This is the hardest integration â€” ML lead should pair with you here | Sign detection WebSocket working |
| 20â€“24 | Add Hume facial expression analysis: for video call mode, periodically send frame snapshots to Hume face API â†’ return facial emotion scores â†’ send to frontend alongside sign predictions | Facial emotion working |
| 24â€“28 | Stress testing and optimization. Handle disconnections gracefully, add connection health checks, optimize frame throughput for sign detection | Stable backend |
| 28â€“32 | Add "blind profile" conversation mode: instead of returning captions + quick-replies, return an audio summary via TTS. Same Whisper + Claude pipeline but with different prompt ("summarize this concisely for someone listening via earbuds") | Blind mode conversation |
| 32â€“36 | Edge cases: what happens when Hume API is slow? Fallback to AFINN text sentiment. What if Groq is down? Fallback to Web Speech API on frontend | Graceful degradation |
| 36â€“42 | Bug fixes, support frontend integration, help with demo recording | Support role |
| 42â€“48 | Demo recording, final fixes, buffer | Done |

**Critical path items for backend lead:**
- Deploy to Railway in hour 2-5, not at the end. Frontend team needs a live URL to connect to.
- The Hume AI WebSocket integration is the most valuable "wow factor" for the demo â€” prioritize it.
- Design Claude/GPT prompts for quick-reply generation early. Bad prompts = useless quick-replies.
- The TTS endpoint should support streaming audio, not batch â€” ElevenLabs supports this natively.

---

### FRONTEND 1 â€” Onboarding + Profile System + App Shell

**Goal: the adaptive profile system that makes the whole app feel like it switches personalities**

| Hours | Task | Output |
|-------|------|--------|
| 0â€“2 | Setup with team (above) | Environment ready |
| 2â€“5 | App shell: Next.js layout, navigation between modes, responsive design. Set up Tailwind with a clean accessibility-first theme (high contrast, large touch targets) | Basic app skeleton |
| 5â€“9 | Onboarding flow (2 screens): Screen 1 â€” "How do you experience the world?" [I'm deaf / hard of hearing] [I'm blind / visually impaired]. Screen 2 â€” confirmation screen adapted to chosen profile. Store profile in React Context | Working onboarding |
| 9â€“13 | Profile-driven rendering system: create a `useProfile()` hook that components use to decide what to render. Build wrapper components: `<ForDeaf>`, `<ForBlind>`, `<ForBoth>` that conditionally render children based on profile | Profile system working |
| 13â€“17 | Home screen with mode selection. Deaf profile sees: [Start Video Call] [Start Conversation Mode] [Settings]. Blind profile sees: large simple buttons, screen reader labels, or auto-reads options via TTS | Home screen for both profiles |
| 17â€“21 | Haptic feedback system: define vibration patterns using Navigator.vibrate(). 2 short = location/status update, 1 long = attention needed, 3 quick = someone is speaking. Wire these into conversation and sign detection events | Haptic patterns working |
| 21â€“26 | Settings page: ability to switch profiles, adjust text size, toggle haptic feedback. Simple but functional | Settings page |
| 26â€“30 | Accessibility audit: screen reader labels (aria-label on everything), keyboard navigation, focus management, color contrast check | Accessible app |
| 30â€“36 | Polish: transitions between screens, loading states, error states, empty states. Make it feel like a real app, not a hackathon prototype | Polished UI |
| 36â€“42 | Help record demo video. Do the onboarding recording and profile-switching demo | Demo segments recorded |
| 42â€“48 | Final polish, bug fixes, buffer | Done |

---

### FRONTEND 2 â€” Video Call Mode (Sign Language)

**Goal: PeerJS video call with sign detection overlay, emotion badges, and TTS output**

| Hours | Task | Output |
|-------|------|--------|
| 0â€“2 | Setup with team (above) | Environment ready |
| 2â€“6 | Basic PeerJS video call: two users connect with a room code, see each other's video, hear each other's audio. Use PeerJS free cloud server. Test in two browser tabs | Video call working |
| 6â€“10 | Add data channel: detected signs and emotions are sent between peers as JSON messages. Display received messages as text overlay on the remote video | Data channel working |
| 10â€“14 | Frame capture pipeline: capture frames from local video element at ~10 FPS, convert to base64, send to backend via WebSocket `/ws/sign-detection`. Display returned sign predictions as text overlay | Sign detection connected to video call |
| 14â€“18 | Sign detection UX: show detected sign as a large caption below the video ("HELLO" with confidence bar). Add animation when a new sign is detected. Buffer repeated detections (don't show "hello hello hello") | Clean sign detection display |
| 18â€“22 | TTS integration: when a sign is detected, send text to `/api/tts` endpoint, play returned audio through the remote peer's speakers. The hearing person hears the sign spoken aloud | Sign â†’ TTS working |
| 22â€“26 | Emotion badges on video: receive Hume facial emotion data from backend, display as small badges on the video feed (ðŸ˜Š happy, ðŸ˜Ÿ concerned, etc.) | Emotion overlay working |
| 26â€“30 | Deaf-profile-specific UI: when profile is deaf, the remote user's audio is also transcribed via Web Speech API and displayed as captions. Two-way communication: deaf user signs â†’ hearing user hears TTS. Hearing user speaks â†’ deaf user reads captions | Full bidirectional communication |
| 30â€“36 | Polish: call controls (mute, end call, toggle sign detection), connection status indicator, reconnection handling | Production-feel video call |
| 36â€“42 | Help record demo video â€” record the video call demo with another team member | Demo segments recorded |
| 42â€“48 | Final polish, bug fixes, buffer | Done |

---

### FRONTEND 3 â€” Conversation Intelligence Mode (In-Person)

**Goal: live captions with tone, quick-replies, and profile-adapted output**

| Hours | Task | Output |
|-------|------|--------|
| 0â€“2 | Setup with team (above) | Environment ready |
| 2â€“6 | "Listen mode" UI: a prominent button to start/stop listening. When active, capture audio via browser MediaRecorder API, stream chunks to backend `/ws/conversation` via WebSocket | Audio streaming to backend |
| 6â€“10 | Caption display: receive transcript + tone from backend, render as scrolling captions. Style like the SenseAI brief described: "[Dr. Lee, speaking carefully]: Your blood sugar is a bit high." Speaker labels are editable (user taps to rename "Speaker 1" â†’ "Dr. Lee") | Live captions with tone |
| 10â€“14 | Quick-reply system: receive quick-reply suggestions from backend, display as tappable buttons below captions. Also show persistent universal replies: [One moment please] [Can you repeat that?]. Add text input for custom responses | Quick-reply buttons working |
| 14â€“18 | TTS for quick-replies: when user taps a quick-reply, send to backend for Claude rephrasing â†’ ElevenLabs TTS â†’ play audio through phone speaker. The doctor/conversant hears a natural-sounding response | Quick-reply â†’ TTS working |
| 18â€“22 | Blind profile adaptation: same conversation backend, but instead of captions, play audio summaries through earbuds. After each speaker turn, TTS whispers a summary: "Your doctor sounds concerned. They said your blood sugar is high." User responds by speaking naturally (their speech goes through the phone mic/speaker, no TTS needed) | Blind mode conversation working |
| 22â€“26 | Tone badges: map Hume emotion scores to visual indicators. Show small colored badges next to speaker names (green = calm, yellow = concerned, red = upset). For blind users, incorporate tone into the audio summary ("your doctor sounds worried") | Tone visualization |
| 26â€“30 | Conversation intro feature: at the start of Listen mode, option to play an introduction through the speaker: "Hello, I'm an accessibility assistant helping [user] communicate today" | Conversation intro |
| 30â€“36 | Polish: smooth scrolling captions, animation on new quick-replies, vibration when new speech detected, clean empty/waiting states | Polished conversation UI |
| 36â€“42 | Help record demo video â€” record the in-person conversation demo (one team member plays "doctor") | Demo segments recorded |
| 42â€“48 | Final polish, bug fixes, buffer | Done |

---

## Integration Checkpoints (CRITICAL â€” don't skip these)

These are moments where multiple people's work needs to connect. Schedule these explicitly.

| Hour | Who | What | If It Fails |
|------|-----|------|-------------|
| 8 | Backend + Frontend 2 + Frontend 3 | Backend is deployed on Railway. Both frontends can connect via WebSocket | Everything is blocked. Backend lead prioritizes this above all else |
| 14 | ML lead + Backend lead | Sign detection WebSocket endpoint works end-to-end (frame in â†’ prediction out) | ML lead tests standalone, backend uses mock predictions for frontend |
| 16 | Frontend 2 + Backend | Video call sends frames to backend, receives sign predictions | Use hardcoded mock predictions if backend isn't ready |
| 18 | Frontend 3 + Backend | Conversation mode sends audio, receives captions + tone + quick-replies | Use Web Speech API as STT fallback, skip tone if Hume isn't ready |
| 24 | ALL | Full integration test. Both modes working end-to-end with real data | This is the "is this demo-able?" checkpoint. Decide what to cut |
| 36 | ALL | Feature freeze. No new features. Only bug fixes and demo recording | Strict. No exceptions. |

---

## Demo Video Structure (5 Minutes)

Since this is a pre-recorded submission, you can carefully script and edit this.

### Script

**[0:00â€“0:15] Hook**
"1 billion people live with disability. Most assistive tech forces them into one mode â€” audio only, or text only. SenseAI adapts to you."

**[0:15â€“0:35] Onboarding â€” Deaf User**
Screen recording: open app, select "I'm deaf / hard of hearing", confirmation with haptic feedback. Show the UI adapting â€” audio off, captions on, large visual elements.

**[0:35â€“1:45] Mode 1 Demo â€” Sign Language Video Call (Deaf User)**
Two team members on a video call. Deaf user signs "Hello" â†’ app detects it â†’ text appears on screen â†’ hearing user hears "Hello" spoken aloud. Hearing user says "Hi, how are you?" â†’ deaf user sees captions with tone badge. Deaf user signs "Thank you." Show the emotion badges updating. This is your WOW moment.

**[1:45â€“3:00] Mode 2 Demo â€” Doctor's Appointment (Deaf User)**
Team member plays doctor. Deaf user activates "Listen mode." Doctor speaks medical jargon â†’ captions appear with tone annotations and simplified text. Quick-reply buttons appear. Deaf user taps "How serious is it?" â†’ app speaks it aloud naturally. Doctor responds. Show the conversational flow.

**[3:00â€“3:15] Transition**
"Same AI. Same app. Different ability."

**[3:15â€“3:30] Onboarding â€” Blind User**
Screen recording (brief): select "I'm blind / visually impaired." App confirms via voice. Phone goes in pocket.

**[3:30â€“4:15] Mode 2 Demo â€” Doctor's Appointment (Blind User)**
Same doctor scenario. But now the user hears audio summaries through earbuds: "Your doctor sounds concerned. They said your blood sugar is high and wants to change your medication. Want to ask anything?" User speaks naturally. Show the contrast with the deaf user's visual experience.

**[4:15â€“4:40] Side-by-side comparison**
Split screen: deaf user's visual experience vs. blind user's audio experience. Same conversation, completely different delivery.

**[4:40â€“5:00] Closing**
"SenseAI doesn't ask you to adapt to technology. It adapts to you." Show the sign language detection, the captions with tone, the quick-replies, the profile switching â€” rapid montage of features.

### Recording Tips
- Record each segment 2-3 times, pick the best take
- Use a quiet room for the doctor conversation demo
- For the video call demo, use two laptops side-by-side showing both perspectives
- Screen record with OBS or built-in screen recording
- Edit with CapCut or DaVinci Resolve (both free)
- Add captions to YOUR demo video (it's an accessibility hackathon!)

---

## Fallback Plan (If Things Go Wrong)

| What breaks | Fallback |
|-------------|----------|
| LSTM accuracy is bad (<80%) | Reduce to 5 highly distinct signs. In the video demo, only show signs that work reliably |
| Hume AI is slow/down | Use AFINN text sentiment on STT transcript. Show "positive/negative/neutral" instead of 48 dimensions |
| ElevenLabs is slow/down | Use browser Web Speech API SpeechSynthesis (free, built-in, less natural sounding) |
| Groq Whisper is slow/down | Use browser Web Speech API SpeechRecognition (free, Chrome only) |
| PeerJS cloud server is down | Run local PeerServer: `npx peerjs --port 9000` |
| WebSocket frame pipeline is too slow | Reduce to 5 FPS. Or pre-record a sign detection demo and splice it into the video |
| Claude/GPT is slow for quick-replies | Pre-define 10 common quick-replies per context (medical, casual). Use rule-based matching instead of LLM |
| Railway deploy fails | Run backend on localhost for demo recording. Both machines on same WiFi |
| Frontend team is blocked by backend | Every frontend component should work with MOCK DATA first. Build with fake responses, connect real backend later |

---

## Pre-Hackathon Checklist (Do This NOW)

- [ ] All 5 team members have Node.js 18+, Python 3.10+, pip, npm installed
- [ ] Shared GitHub repo created with monorepo structure
- [ ] All API keys obtained and tested:
  - [ ] Hume AI (dev.hume.ai â€” sign up, get API key, test the playground)
  - [ ] ElevenLabs (elevenlabs.io â€” sign up, get API key, test TTS)
  - [ ] Groq (console.groq.com â€” sign up, get API key, test Whisper)
  - [ ] Anthropic or OpenAI (for Claude/GPT)
- [ ] ML lead has tested MediaPipe Holistic on their machine with webcam
- [ ] Frontend team has scaffolded Next.js app and deployed to Vercel
- [ ] Backend lead has scaffolded FastAPI app and deployed to Railway
- [ ] Everyone has read this sprint plan and knows their hour-by-hour tasks
- [ ] Someone has downloaded OBS for screen recording
- [ ] Team has agreed on a Slack/Discord channel for async communication during the hackathon
