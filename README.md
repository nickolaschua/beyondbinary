# BeyondBinary

Accessibility-first communication platform that bridges the gap between deaf, blind, deafblind, and mute users through real-time AI. Combines ASL sign language detection, speech-to-text, text-to-speech, emotional tone analysis, braille output, and peer-to-peer video calling into a single live workspace.

## The Problem

Current assistive technologies focus on single-modality solutions â€” speech-to-text, basic sign recognition, or simple navigation aids â€” that don't address the complex, multi-layered needs of users with disabilities. These fragmented tools fail to account for regional sign language variations, contextual nuances, and the reality that many users need multiple modalities working together simultaneously.

BeyondBinary tackles this by combining vision, audio, text, haptics, and AI into one cohesive system that adapts to individual needs. A deaf user gets large captions with emotional tone indicators. A blind user gets speech narration and braille. A deafblind user gets both. Rather than forcing users to stitch together separate tools, BeyondBinary provides a single workspace where all modalities work in concert.

## How It Works

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚           Live Workspace (Frontend)       â”‚
                    â”‚                                          â”‚
                    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
                    â”‚  â”‚ Webcam  â”‚  â”‚ Captions  â”‚  â”‚Braille â”‚  â”‚
                    â”‚  â”‚ + Sign  â”‚  â”‚ + Tone    â”‚  â”‚Display â”‚  â”‚
                    â”‚  â”‚Detectionâ”‚  â”‚ Feed      â”‚  â”‚(6-dot) â”‚  â”‚
                    â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â”‚
                    â”‚       â”‚             â”‚            â”‚        â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚             â”‚            â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”     â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”    â”‚
              â”‚  ML Server    â”‚     â”‚  Backend    â”‚    â”‚
              â”‚  MediaPipe â†’  â”‚     â”‚  Groq STT   â”‚    â”‚
              â”‚  LSTM â†’ Sign  â”‚     â”‚  Hume Tone  â”‚    â”‚
              â”‚  (port 8765)  â”‚     â”‚  11Labs TTS â”‚    â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚  Claude AI  â”‚    â”‚
                                    â”‚  (port 8000)â”‚    â”‚
                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
                                          â”‚            â”‚
                                    â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
                                    â”‚   UEB Grade 1 Braille  â”‚
                                    â”‚   Translation Engine   â”‚
                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Accessibility Profiles

Users select a profile at onboarding. Each profile activates a different combination of input/output channels:

| Profile | Receives | Sends | Special Features |
|---------|----------|-------|------------------|
| **Deaf** | Large captions, sign interpretation, tone indicators | Text, message cards | Tone emoji badges (ðŸ˜Š ðŸ˜  ðŸ˜Ÿ), visual-first layout |
| **Blind** | Speech narration, braille output, tone identification | Text-to-speech | Audio guidance on every page, ElevenLabs voice |
| **Deafblind** | Braille (always-on), optional audio, tone labels | Text-to-speech, message cards | 12-cell braille display, extra-large 5xl text |
| **Mute** | Captions, sign interpretation, audio context | Text-to-speech, text output | Quick reply buttons for fast responses |

## Features

### ASL Sign Detection
Real-time detection of 12 ASL signs via webcam at 5 FPS:

> Hello, Thank You, Help, Yes, No, Please, Sorry, I Love You, Stop, More, How Are You, Good

Pipeline: webcam frame â†’ MediaPipe Holistic (1662 landmarks) â†’ 30-frame sliding window â†’ LSTM classifier â†’ stability filter (5 consecutive frames) â†’ confirmed sign.

### Voice + Tone Intelligence
- **Speech-to-text** via Groq Whisper (~200ms latency), OpenAI fallback
- **Emotional tone analysis** via Hume AI prosody (~300ms), AFINN sentiment fallback
- **Text-to-speech** via ElevenLabs multilingual v2, Web Speech API fallback
- **Quick replies** generated by Claude based on conversation context

### Braille Display
Visual 6-dot UEB Grade 1 braille cells rendered in the browser. 12-cell scrolling display converts conversation text to braille in real-time. Supports aâ€“z, 0â€“9, and common punctuation with number indicator prefix.

### Video Calling
WebRTC peer-to-peer video with STUN/TURN relay. Signaling through backend WebSocket. Camera toggle, local/remote stream display.

## Tech Stack

| Layer | Technology | Port |
|-------|-----------|------|
| Frontend | Next.js 16, React 19, TypeScript, Tailwind CSS 4 | 3000 |
| Backend | FastAPI, Python 3.12 | 8000 |
| ML Server | TensorFlow 2.16, MediaPipe 0.10.21, LSTM | 8765 |

### External Services

| Service | Provider | Purpose | Fallback |
|---------|----------|---------|----------|
| Speech-to-text | Groq Whisper | Audio transcription | OpenAI Whisper |
| Text-to-speech | ElevenLabs | Voice synthesis | Web Speech API |
| Tone analysis | Hume AI | Prosody/emotion detection | AFINN sentiment |
| AI intelligence | Anthropic Claude | Quick replies, jargon simplification | â€” |

## Quick Start

### 1. ML Server

```bash
# Requires Python 3.12 (3.13 is NOT compatible with MediaPipe)
py -3.12 -m venv ml/venv
ml\venv\Scripts\pip.exe install -r ml/requirements.txt

# Verify environment
cd ml && venv\Scripts\python.exe test_setup.py

# Start WebSocket server on port 8765
cd ml && venv\Scripts\python.exe ws_server.py
```

### 2. Backend

```bash
cd backend
python -m venv venv
venv\Scripts\pip.exe install -r requirements.txt

# Copy .env.example and fill in your API keys
cp .env.example .env
# Required: GROQ_API_KEY, ELEVENLABS_API_KEY, ANTHROPIC_API_KEY, HUME_API_KEY
# Optional: OPENAI_API_KEY (STT fallback)

python run_dev.sh
```

### 3. Frontend

```bash
cd senseai-frontend
npm install
npm run dev
```

Open `http://localhost:3000` â†’ select your accessibility profile â†’ enter live workspace.

## Project Structure

```
beyondbinary/
â”œâ”€â”€ ml/                          # ASL detection pipeline
â”‚   â”œâ”€â”€ ws_server.py             # WebSocket inference server
â”‚   â”œâ”€â”€ inference.py             # Real-time prediction engine
â”‚   â”œâ”€â”€ landmarks.py             # MediaPipe landmark extraction
â”‚   â”œâ”€â”€ preprocessing.py         # Data augmentation + preprocessing
â”‚   â”œâ”€â”€ train_model.py           # LSTM training script
â”‚   â”œâ”€â”€ model_architecture.py    # Keras model definition
â”‚   â”œâ”€â”€ config.py                # Signs, thresholds, paths
â”‚   â””â”€â”€ tests/                   # 214+ pytest tests
â”œâ”€â”€ backend/                     # API gateway + service orchestration
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py              # FastAPI app entry point
â”‚   â”‚   â”œâ”€â”€ config.py            # Environment + settings
â”‚   â”‚   â”œâ”€â”€ routers/             # Endpoint handlers (STT, TTS, tone, braille, conversation)
â”‚   â”‚   â””â”€â”€ services/            # External API integrations (Groq, Hume, ElevenLabs, Claude)
â”‚   â”œâ”€â”€ .env.example             # API key template
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ senseai-frontend/            # Accessible UI
â”‚   â”œâ”€â”€ src/app/                 # Next.js pages (onboarding, session, settings)
â”‚   â”œâ”€â”€ src/components/          # React components (LiveWorkspace, BrailleCell, VideoCall, ...)
â”‚   â”œâ”€â”€ src/hooks/               # useWebSocket, useWebRTC, useVoiceCommands
â”‚   â””â”€â”€ src/lib/                 # API client, profile config, braille mapping
â””â”€â”€ docs/
    â”œâ”€â”€ WEBSOCKET.md             # ML WebSocket protocol spec
    â””â”€â”€ BACKEND_INTEGRATION.md   # Frontend â†” Backend integration guide
```

## Running Tests

```bash
# ML pipeline (214+ tests)
ml\venv\Scripts\python.exe -m pytest ml/tests/ -v
```

## License

All rights reserved.
