---
phase: 19-frontend-integration
plan: 01
type: execute
---

<objective>
Wire the ML sign-detection server into the Next.js frontend so sentence assembly, sign predictions, and sentence completions display correctly end-to-end.

Purpose: The ML WebSocket server now sends `sentence_in_progress` on every prediction and `sentence_complete` when a pause is detected, but the frontend ignores both. Additionally, `constants.ts` has the wrong default for `API_URL` (points at ML port 8001 instead of backend port 8000), breaking conversation/TTS.

Output: Frontend correctly separates backend (8000) from ML (8001) URLs, displays live sentence assembly, and handles completed sentences.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

**Key source files:**
@senseai-frontend/src/lib/constants.ts
@senseai-frontend/src/components/LiveWorkspace.tsx
@ml/ws_server.py (reference — sentence assembly logic, message formats)
@docs/WEBSOCKET.md (reference — protocol spec)

**Established patterns:**
- LiveWorkspace receives `apiUrl` (backend) and `wsUrl` (ML) as props from live/*/page.tsx
- Sign detection messages flow through `onMessage` callback with `WsPayload` type union
- Conversation messages flow through `handleConvMessage` callback
- `transcript` state array holds individual signs; `conversationTranscript` holds speech lines
- Braille and TTS are triggered on new signs via existing hooks

**Constraining decisions:**
- LSTM model only (no transformer) — ws_server.py sends `sign_prediction` with `sentence_in_progress` field
- Sentence timeout is 2 seconds (configurable via `SENSEAI_SENTENCE_TIMEOUT` env var)
- `sentence_complete` message has `{type: "sentence_complete", sentence: "...", word_count: N}`

**Branch:** All work must be done on the `ml` branch (user instruction).
</context>

<tasks>

<task type="auto">
  <name>Task 1: Fix API_URL default and add sentence_complete to WsPayload</name>
  <files>senseai-frontend/src/lib/constants.ts, senseai-frontend/src/components/LiveWorkspace.tsx</files>
  <action>
  1. In `constants.ts`: Change `API_URL` default from `"http://localhost:8001"` to `"http://localhost:8000"`. This separates the backend API (port 8000) from the ML WebSocket server (port 8001). Leave `WS_URL` as `"ws://localhost:8001"` (correct for ML server).

  2. In `LiveWorkspace.tsx`: Add `sentence_complete` to the `WsPayload` type union:
     `| { type: "sentence_complete"; sentence: string; word_count: number }`
     This matches the message format sent by `ml/ws_server.py` when a signing pause exceeds `SENTENCE_TIMEOUT`.
  </action>
  <verify>TypeScript compilation succeeds (`npx tsc --noEmit` in senseai-frontend). WsPayload union includes all 4 message types.</verify>
  <done>`API_URL` defaults to port 8000, `WS_URL` defaults to port 8001, `sentence_complete` is in WsPayload type.</done>
</task>

<task type="auto">
  <name>Task 2: Display sentence_in_progress and handle sentence_complete in LiveWorkspace</name>
  <files>senseai-frontend/src/components/LiveWorkspace.tsx</files>
  <action>
  1. Add a `sentenceInProgress` state variable (string, initially empty).

  2. In the sign-detection `onMessage` callback (the one handling `WsPayload`):
     - After processing `sign_prediction`, update `sentenceInProgress` from `payload.sentence_in_progress ?? ""`.
     - Add a handler for `sentence_complete`: when received, add `payload.sentence` to the `transcript` array as a full sentence (distinguishable from individual signs), reset `sentenceInProgress` to empty, and if `useBraille` is active, append the sentence to braille cells.

  3. In the JSX "Current interpretation" section (around line 501-511):
     - Below the `latestSign` display and confidence, render `sentenceInProgress` when non-empty as a subtitle line showing the sentence being assembled. Use a muted style (e.g., `text-slate-400 text-lg`) to distinguish from the primary sign.
     - Example: `{sentenceInProgress && <p className="mt-1 text-lg text-slate-400">{sentenceInProgress}</p>}`

  4. Do NOT change the conversation WebSocket handler (`handleConvMessage`) — sentence assembly only applies to sign detection.
  </action>
  <verify>
  - TypeScript compiles without errors (`npx tsc --noEmit` in senseai-frontend)
  - `sentenceInProgress` state variable exists and is updated from sign_prediction messages
  - `sentence_complete` handler appends to transcript and resets sentenceInProgress
  - UI shows assembling sentence below the current sign
  </verify>
  <done>
  - `sentence_in_progress` text renders below current sign in "Current interpretation" section
  - `sentence_complete` messages append the full sentence to the transcript array and reset the in-progress display
  - All existing functionality preserved (braille, TTS, quick replies still work)
  </done>
</task>

</tasks>

<verification>
Before declaring phase complete:
- [ ] `npx tsc --noEmit` passes in senseai-frontend (no TypeScript errors)
- [ ] `API_URL` defaults to `http://localhost:8000` in constants.ts
- [ ] `WS_URL` defaults to `ws://localhost:8001` in constants.ts
- [ ] `WsPayload` type includes `sentence_complete` message type
- [ ] `sentenceInProgress` state is set from `sign_prediction` messages
- [ ] `sentence_complete` messages update transcript array
- [ ] Sentence-in-progress text renders in the "Current interpretation" section
- [ ] No regressions in existing sign detection or conversation flows
</verification>

<success_criteria>

- All tasks completed
- All verification checks pass
- No TypeScript errors introduced
- Frontend correctly routes API calls to backend (8000) and sign detection to ML (8001)
- Live sentence assembly displays as signs are detected
- Completed sentences appear in transcript
</success_criteria>

<output>
After completion, create `.planning/phases/19-frontend-integration/19-01-SUMMARY.md`
</output>
