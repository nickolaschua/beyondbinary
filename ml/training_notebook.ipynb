{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "source": "# Cell 1: Upload and extract MP_Data.zip directly\nimport os, zipfile\nfrom google.colab import files\n\nprint('Select your MP_Data.zip file...')\nuploaded = files.upload()\n\nzip_name = list(uploaded.keys())[0]\nassert zip_name.endswith('.zip'), f'Expected a .zip file, got: {zip_name}'\n\nprint(f'\\nUploaded: {zip_name} ({len(uploaded[zip_name]) / (1024*1024):.1f} MB)')\n\nwith zipfile.ZipFile(zip_name, 'r') as z:\n    z.extractall('/content/')\n\nDATA_PATH = '/content/MP_Data'\nMODELS_DIR = '/content/models'\nos.makedirs(MODELS_DIR, exist_ok=True)\n\nprint(f'\\nDATA_PATH: {DATA_PATH}')\nprint(f'Data exists: {os.path.isdir(DATA_PATH)}')\nprint(f'Signs found: {sorted(os.listdir(DATA_PATH))}')",
   "metadata": {
    "id": "cell-1-mount-drive",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Cell 2: Imports\nimport numpy as np\nimport os\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\nfrom tensorflow.keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set random seeds for reproducibility\nnp.random.seed(42)\ntf.random.set_seed(42)",
   "metadata": {
    "id": "cell-2-imports"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Cell 3: Configuration\n# DATA_PATH set in Cell 1\nACTIONS = np.array([\n    'Hello', 'Thank_You', 'Help', 'Yes', 'No',\n    'Please', 'Sorry', 'I_Love_You', 'Stop', 'More'\n])\nNUM_SEQUENCES = 30\nSEQUENCE_LENGTH = 30\nlabel_map = {label: num for num, label in enumerate(ACTIONS)}\n\nprint(f'Actions ({len(ACTIONS)}): {list(ACTIONS)}')\nprint(f'Sequences per action: {NUM_SEQUENCES}')\nprint(f'Frames per sequence: {SEQUENCE_LENGTH}')\nprint(f'Expected total sequences: {len(ACTIONS) * NUM_SEQUENCES}')",
   "metadata": {
    "id": "cell-3-config"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Cell 4: Load and Prepare Data (auto-discovers all sequences)\nsequences = []\nlabels = []\nskipped = 0\n\nfor action in ACTIONS:\n    action_dir = os.path.join(DATA_PATH, action)\n    if not os.path.isdir(action_dir):\n        print(f'WARNING: {action_dir} not found!')\n        continue\n\n    # Auto-discover all sequence directories\n    seq_dirs = sorted([d for d in os.listdir(action_dir)\n                       if os.path.isdir(os.path.join(action_dir, d)) and d.isdigit()],\n                      key=int)\n\n    for seq_name in seq_dirs:\n        window = []\n        valid = True\n        for frame_idx in range(SEQUENCE_LENGTH):\n            frame_path = os.path.join(action_dir, seq_name, f'{frame_idx}.npy')\n            if not os.path.isfile(frame_path):\n                valid = False\n                break\n            try:\n                frame = np.load(frame_path)\n                if frame.shape != (1662,):\n                    print(f'Warning: unexpected shape {frame.shape} in {frame_path}')\n                    valid = False\n                    break\n                window.append(frame)\n            except Exception as e:\n                print(f'Error loading {frame_path}: {e}')\n                valid = False\n                break\n\n        if valid and len(window) == SEQUENCE_LENGTH:\n            sequences.append(window)\n            labels.append(label_map[action])\n        else:\n            skipped += 1\n\nX = np.array(sequences)\ny = to_categorical(np.array(labels), num_classes=len(ACTIONS))\n\nprint(f'X shape: {X.shape}')\nprint(f'y shape: {y.shape}')\nprint(f'Skipped sequences: {skipped}')\n\n# Per-class counts\ny_int = np.argmax(y, axis=1)\nfor i, action in enumerate(ACTIONS):\n    print(f'  {action}: {np.sum(y_int == i)} sequences')",
   "metadata": {
    "id": "cell-4-load-data"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Cell 5: Train/Test Split (90/10 with stratification)\n",
    "y_integers = np.argmax(y, axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.1, random_state=42, stratify=y_integers\n",
    ")\n",
    "\n",
    "print(f'Training samples: {X_train.shape[0]}')\n",
    "print(f'Test samples:     {X_test.shape[0]}')\n",
    "print(f'Train shape: {X_train.shape}')\n",
    "print(f'Test shape:  {X_test.shape}')"
   ],
   "metadata": {
    "id": "cell-5-split"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Cell 6: Build LSTM Model\n# CRITICAL: Do not change this architecture\n# - LSTM activation MUST be 'tanh' (not relu)\n# - BatchNormalization after each LSTM layer\n# - Dropout(0.2) for regularization\n\nmodel = Sequential([\n    LSTM(64, return_sequences=True, activation='tanh', input_shape=(30, 1662)),\n    BatchNormalization(),\n    Dropout(0.2),\n\n    LSTM(128, return_sequences=True, activation='tanh'),\n    BatchNormalization(),\n    Dropout(0.2),\n\n    LSTM(64, return_sequences=False, activation='tanh'),\n    BatchNormalization(),\n    Dropout(0.2),\n\n    Dense(64, activation='relu'),\n    Dense(32, activation='relu'),\n    Dense(len(ACTIONS), activation='softmax')\n])\n\nmodel.compile(\n    optimizer='adam',\n    loss='categorical_crossentropy',\n    metrics=['categorical_accuracy']\n)\nmodel.summary()",
   "metadata": {
    "id": "cell-6-model"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Cell 7: Train\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_categorical_accuracy',\n",
    "    patience=30,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "checkpoint = ModelCheckpoint(\n",
    "    'best_model.h5',\n",
    "    monitor='val_categorical_accuracy',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "tb_callback = TensorBoard(log_dir='./logs')\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=200,\n",
    "    batch_size=16,\n",
    "    validation_split=0.15,\n",
    "    callbacks=[early_stop, checkpoint, tb_callback],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(f'\\nBest validation accuracy: {max(history.history[\"val_categorical_accuracy\"]):.4f}')"
   ],
   "metadata": {
    "id": "cell-7-train"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Cell 8: Evaluate on TEST set\nfrom tensorflow.keras.models import load_model\n\n# Load the best model from checkpoint\nbest_model = load_model('best_model.h5')\n\n# Evaluate on X_test (NOT X_train)\ntest_loss, test_acc = best_model.evaluate(X_test, y_test, verbose=0)\nprint(f'Test Loss:     {test_loss:.4f}')\nprint(f'Test Accuracy: {test_acc:.4f}')\n\n# Classification report\ny_pred = best_model.predict(X_test, verbose=0)\ny_pred_classes = np.argmax(y_pred, axis=1)\ny_true_classes = np.argmax(y_test, axis=1)\n\nprint('\\nClassification Report:')\nprint(classification_report(\n    y_true_classes,\n    y_pred_classes,\n    target_names=ACTIONS.tolist(),\n    zero_division=0\n))\n\n# Confusion matrix heatmap\ncm = confusion_matrix(y_true_classes, y_pred_classes)\nplt.figure(figsize=(12, 10))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n            xticklabels=ACTIONS, yticklabels=ACTIONS)\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Confusion Matrix')\nplt.tight_layout()\nplt.savefig('confusion_matrix.png', dpi=150)\nplt.show()\nprint('Confusion matrix saved to confusion_matrix.png')",
   "metadata": {
    "id": "cell-8-evaluate"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Cell 9: Training History Plots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy plot\n",
    "axes[0].plot(history.history['categorical_accuracy'], label='Train Accuracy')\n",
    "axes[0].plot(history.history['val_categorical_accuracy'], label='Val Accuracy')\n",
    "axes[0].set_title('Model Accuracy')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Loss plot\n",
    "axes[1].plot(history.history['loss'], label='Train Loss')\n",
    "axes[1].plot(history.history['val_loss'], label='Val Loss')\n",
    "axes[1].set_title('Model Loss')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png', dpi=150)\n",
    "plt.show()\n",
    "print('Training history saved to training_history.png')"
   ],
   "metadata": {
    "id": "cell-9-history"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Cell 10: Save + Download model\nMODEL_PATH = os.path.join(MODELS_DIR, 'action_model.h5')\n\nbest_model.save(MODEL_PATH)\nnp.save('actions.npy', ACTIONS)\n\n# Download directly to your PC\nfrom google.colab import files\nfiles.download(MODEL_PATH)\nfiles.download('actions.npy')\nfiles.download('confusion_matrix.png')\nprint('Downloads started â€” place action_model.h5 in ml/models/ locally')",
   "metadata": {
    "id": "cell-10-save"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}